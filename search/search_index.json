{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Northumbria University Research Computing Community \u00b6 This is the website for the Northumbria University Research Computing Community. You can find information about upcoming events , sign-up to our newsletter , browse past newsletters, or access documentation for the NU Supercomputer Oswald . What is the Research Computing Community? \u00b6 Many researchers at Northumbria use computing for their research. This includes High-Performance Computing (HPC), Research Software Engineering (RSE), developers and users. Many of us also use similar tools, face similar problems, and teach similar topics - even if from different research domains. We hope that by sharing our experiences, our tips and tricks, our trials and tribulations, we will support and grow the research computing community at Northumbria. Contributing to the community \u00b6 If you have any research computing-related news, ideas, projects, or events you want to share with the community, we'd love to hear from you. Please get in touch with ??? to have your submission considered for inclusion in our next newsletter. For example: Have you been using software or computing as part of your research and want to share outcomes or techniques? Have you been working on a software/computing project to assist in your or others' research that others may find useful? Perhaps you've come across some news or ideas about research software/computing that you want to share with the community? If you're hosting a research computing-related event, we would also like to hear about it. Please get in touch with ??? and we can add your event to our list. Also, if you find something incorrect, out of date or missing in this website, feel free to get in touch with ???. Or, if you have the time and skills to do so, you can even head over to this website's GitHub repository , make the edits yourself and create a Pull Request. We appreciate all contributions! Todo Add in the name(s) and email address(es) of one or more people willing to handle these kinds of requests.","title":"Home"},{"location":"#northumbria-university-research-computing-community","text":"This is the website for the Northumbria University Research Computing Community. You can find information about upcoming events , sign-up to our newsletter , browse past newsletters, or access documentation for the NU Supercomputer Oswald .","title":"Northumbria University Research Computing Community"},{"location":"#what-is-the-research-computing-community","text":"Many researchers at Northumbria use computing for their research. This includes High-Performance Computing (HPC), Research Software Engineering (RSE), developers and users. Many of us also use similar tools, face similar problems, and teach similar topics - even if from different research domains. We hope that by sharing our experiences, our tips and tricks, our trials and tribulations, we will support and grow the research computing community at Northumbria.","title":"What is the Research Computing Community?"},{"location":"#contributing-to-the-community","text":"If you have any research computing-related news, ideas, projects, or events you want to share with the community, we'd love to hear from you. Please get in touch with ??? to have your submission considered for inclusion in our next newsletter. For example: Have you been using software or computing as part of your research and want to share outcomes or techniques? Have you been working on a software/computing project to assist in your or others' research that others may find useful? Perhaps you've come across some news or ideas about research software/computing that you want to share with the community? If you're hosting a research computing-related event, we would also like to hear about it. Please get in touch with ??? and we can add your event to our list. Also, if you find something incorrect, out of date or missing in this website, feel free to get in touch with ???. Or, if you have the time and skills to do so, you can even head over to this website's GitHub repository , make the edits yourself and create a Pull Request. We appreciate all contributions! Todo Add in the name(s) and email address(es) of one or more people willing to handle these kinds of requests.","title":"Contributing to the community"},{"location":"events/events/","text":"Research Computing Events at Northumbria \u00b6 Thursday 21st October - Northumbria Research Computing Launch event \u00b6 https://rsc-northumbria.github.io/RSC-Northumbria-launch/ This is your chance to meet other researchers from across Northumbria University who use and develop research software. We will also have a keynote from Mike Croucher, who is a Customer Success Engineer at MathWorks. Mike works with researchers and educators on many different aspects of research computing. His interests center around research software engineering, high performance and cloud computing and various aspects of machine learning, mathematics and science. He has over 20 years of experience in the field. Over lunch (which is provided) we will have discussion tables to explore various questions, including: where next for the Northumbria research computing community? do we want to run a regular newsletter? how can we best support research computing at Northumbria? how might we want to work with existing regional and national research computing networks?","title":"All Events"},{"location":"events/events/#research-computing-events-at-northumbria","text":"","title":"Research Computing Events at Northumbria"},{"location":"events/events/#thursday-21st-october-northumbria-research-computing-launch-event","text":"https://rsc-northumbria.github.io/RSC-Northumbria-launch/ This is your chance to meet other researchers from across Northumbria University who use and develop research software. We will also have a keynote from Mike Croucher, who is a Customer Success Engineer at MathWorks. Mike works with researchers and educators on many different aspects of research computing. His interests center around research software engineering, high performance and cloud computing and various aspects of machine learning, mathematics and science. He has over 20 years of experience in the field. Over lunch (which is provided) we will have discussion tables to explore various questions, including: where next for the Northumbria research computing community? do we want to run a regular newsletter? how can we best support research computing at Northumbria? how might we want to work with existing regional and national research computing networks?","title":"Thursday 21st October - Northumbria Research Computing Launch event"},{"location":"newsletter/2021-10/","text":"HPC/RSE Newsletter - October 2021 \u00b6 todo","title":"October"},{"location":"newsletter/2021-10/#hpcrse-newsletter-october-2021","text":"todo","title":"HPC/RSE Newsletter - October 2021"},{"location":"newsletter/signup/","text":"Our newsletter is a roundup of the latest news, ideas, projects, and events from across the wider research computing community. #mc_embed_signup { clear: left; font-size: 14px; } /* Use visible colours on light and dark backgrounds (it will have a border too) */ #mc_embed_signup input { background-color: #eee; color: #000; } /* Behave like a link (see mkdocs-material for theme colour vars) */ #mc_embed_signup input[type=\"submit\"] { background-color: var(--md-primary-fg-color); color: var(--md-primary-bg-color); } #mc_embed_signup input[type=\"submit\"]:hover, #mc_embed_signup input[type=\"submit\"]:focus, #mc_embed_signup input[type=\"submit\"]:active { background-color: var(--md-accent-fg-color); color: var(--md-accent-bg-color); } * indicates required Email Address *","title":"Sign Up"},{"location":"oswald/what-is-oswald/","text":"What is Oswald? \u00b6 Oswald is one of Northumbria University's HPC clusters, which is available to Northumbria research staff in the Faculty of Engineering and Environment. Oswald is named after Oswald the King of Northumbria (c604-5 August 642). What is Supercomputing? \u00b6 Supercomputing is defined as the most powerful form of computing at the present time. Supercomputers are therefore the biggest, fastest computers that exist right now. The definition of 'supercomputer' changes over time as supercomputers get bigger and faster. As a rule of thumb, a supercomputer is at least 100 times as powerful as a PC. Supercomputing is also known as High Performance Computing (HPC) or High End Computing (HEC). HPC has two main advantages: Speed : Many problems that are interesting to scientists and engineers would take a very long time to run on a PC - months or even years. But a problem that would take a month on a PC might only take a few hours on a supercomputer. Size : Many problems that are interesting to scientists and engineers can\u2019t fit on a PC - usually because they need more than a few GB of RAM, or more than a few 100GB of disk space. HPC takes a divide and conquer approach to solve big problems \u2013 dividing a problem into parts, assigning to workers, and then combining the results as illustrated below. Clusters \u00b6 Modern supercomputers are usually cluster computers - where multiple (already quite powerful) computers are connected together to form a powerful compute resource. As such, 'cluster' is often used as another term for supercomputer in the context of HPC. A cluster consists of a head (master) node and several small(er) computers called compute nodes, all connected together with a high speed interconnecting network (Oswald uses a 1GB/10GB/IB interconnect). Software allows the head and compute nodes to communicate over the interconnection network. The diagram below shows a typical cluster with Gigabit Ethernet interconnect. A resource manager on the head node allocates work to the compute nodes. What Are Supercomputers Used For? \u00b6 Supercomputers are used in a wide range of fields of research. Oswald is commonly used in the following research areas: Solar Physics simulation and analysis Climate modelling Ice sheet and glacier modelling Physics analysis and simulations Mechanical Engineering modelling using CFD AI and Deep Learning Oswald Hardware Specifications \u00b6 Num Type CPU GPU RAM Storage 1 Head Node Dual Intel Xeon E5-2630 v4 10 Core 2.2GHz (20 cores total) 64GB 1TB SSD, 185TB HDD 32 Compute Node Dual Intel Xeon E5-2680 v4 14 Core 2.4GHz (28 cores total) 64GB 120GB SSD 1 GPU Node Intel Skylake SKL-SP 4112 4 Core 2.6GHz 2 x NVIDIA Tesla V100 (each with 640 Tensor cores, 5120 CUDA cores) 64GB 240GB SSD 1 Visualisation Node Dual Intel Xeon E5-2630 v4 10 Core 2.2GHz (20 cores total) NVIDIA Tesla M60 (4096 CUDA Cores) 256GB 240GB SSD, 12TB HDD Todo /local on some compute nodes is 184GB (see Storage and Filesystems ) - do these nodes have larger drives than 120GB? Oswald also has: An Intel Omnipath 100Gb interconnect 88TB of Lustre parallel storage Hyperthreading enabled on all compute nodes, allowing 56 threads to run Note Hyperthreading does not mean a doubling of speed, although some jobs will run faster than just using 28 cores.","title":"What is Oswald?"},{"location":"oswald/what-is-oswald/#what-is-oswald","text":"Oswald is one of Northumbria University's HPC clusters, which is available to Northumbria research staff in the Faculty of Engineering and Environment. Oswald is named after Oswald the King of Northumbria (c604-5 August 642).","title":"What is Oswald?"},{"location":"oswald/what-is-oswald/#what-is-supercomputing","text":"Supercomputing is defined as the most powerful form of computing at the present time. Supercomputers are therefore the biggest, fastest computers that exist right now. The definition of 'supercomputer' changes over time as supercomputers get bigger and faster. As a rule of thumb, a supercomputer is at least 100 times as powerful as a PC. Supercomputing is also known as High Performance Computing (HPC) or High End Computing (HEC). HPC has two main advantages: Speed : Many problems that are interesting to scientists and engineers would take a very long time to run on a PC - months or even years. But a problem that would take a month on a PC might only take a few hours on a supercomputer. Size : Many problems that are interesting to scientists and engineers can\u2019t fit on a PC - usually because they need more than a few GB of RAM, or more than a few 100GB of disk space. HPC takes a divide and conquer approach to solve big problems \u2013 dividing a problem into parts, assigning to workers, and then combining the results as illustrated below.","title":"What is Supercomputing?"},{"location":"oswald/what-is-oswald/#clusters","text":"Modern supercomputers are usually cluster computers - where multiple (already quite powerful) computers are connected together to form a powerful compute resource. As such, 'cluster' is often used as another term for supercomputer in the context of HPC. A cluster consists of a head (master) node and several small(er) computers called compute nodes, all connected together with a high speed interconnecting network (Oswald uses a 1GB/10GB/IB interconnect). Software allows the head and compute nodes to communicate over the interconnection network. The diagram below shows a typical cluster with Gigabit Ethernet interconnect. A resource manager on the head node allocates work to the compute nodes.","title":"Clusters"},{"location":"oswald/what-is-oswald/#what-are-supercomputers-used-for","text":"Supercomputers are used in a wide range of fields of research. Oswald is commonly used in the following research areas: Solar Physics simulation and analysis Climate modelling Ice sheet and glacier modelling Physics analysis and simulations Mechanical Engineering modelling using CFD AI and Deep Learning","title":"What Are Supercomputers Used For?"},{"location":"oswald/what-is-oswald/#oswald-hardware-specifications","text":"Num Type CPU GPU RAM Storage 1 Head Node Dual Intel Xeon E5-2630 v4 10 Core 2.2GHz (20 cores total) 64GB 1TB SSD, 185TB HDD 32 Compute Node Dual Intel Xeon E5-2680 v4 14 Core 2.4GHz (28 cores total) 64GB 120GB SSD 1 GPU Node Intel Skylake SKL-SP 4112 4 Core 2.6GHz 2 x NVIDIA Tesla V100 (each with 640 Tensor cores, 5120 CUDA cores) 64GB 240GB SSD 1 Visualisation Node Dual Intel Xeon E5-2630 v4 10 Core 2.2GHz (20 cores total) NVIDIA Tesla M60 (4096 CUDA Cores) 256GB 240GB SSD, 12TB HDD Todo /local on some compute nodes is 184GB (see Storage and Filesystems ) - do these nodes have larger drives than 120GB? Oswald also has: An Intel Omnipath 100Gb interconnect 88TB of Lustre parallel storage Hyperthreading enabled on all compute nodes, allowing 56 threads to run Note Hyperthreading does not mean a doubling of speed, although some jobs will run faster than just using 28 cores.","title":"Oswald Hardware Specifications"},{"location":"oswald/quickstart/accessing-oswald/","text":"Accessing Oswald \u00b6 Getting an Account \u00b6 To access the Oswald cluster you need to have an account set up by the cluster system administrators. Log a ticket with the IT Helpline to request this and you will be provided with a username and password by email (your 'registration email'). Note Throughout the following documentation, replace <username> with the username you were given in your registration email. Logging In \u00b6 SSH Clients \u00b6 Like most supercomputers, Oswald allows you to log in using Secure Shell (SSH). SSH is available for various operating systems: Unix/Linux : The OpenSSH client is pre-installed on many Linux distributions. If the OpenSSH client is not installed on your system: If using your own machine, you can install it with your distribution's package manager (eg. sudo apt install openssh-client on Ubuntu-based systems). If using a university machine, log a ticket with the IT Helpline to have it set up on your machine. Once installed, log in using: ssh <username>@oswald Mac : The OpenSSH client is pre-installed on most Macs. Log in using: ssh <username>@oswald Windows : Multiple applications for Windows allow using SSH, including: Putty MobaXterm If none of this software is installed on your machine: If using your own machine, you can download and install the application of your choice from the pages linked above. If using a university machine, log a ticket with the IT Helpline to have the required software set up on your machine. Refer to the documentation for these applications for how to connect using them. The 'server address' you need to connect to is oswald . Todo How can users connect to Oswald from outside the university, where the oswald hostname is not already set up? Note : Remember to also update Configuring SSH below. Todo Add detail about setting up graphical ssh clients for Oswald. Security and Authentication \u00b6 After running the above command (or before or while attempting to log in through your SSH client application), you will be prompted for a password. You should use the password you received in your registration email. The first time you log in, you may also be asked to accept the \"host key\" of Oswald's head node. You can accept this. Configuring SSH \u00b6 When using the above Unix/Linux/Mac commands, having to repeat them every time you wish to log in or transfer files can become tedious. OpenSSH allows using a configuration file (usually located at ~/.ssh/config ) to save some of the information for future logins. In that file, you can add the following: Host oswald HostName oswald User <username> After that, you can provide the server address (which usually looks like <username>@oswald ) as simply oswald , such as in: ssh oswald scp <file-name> oswald:<destination-directory> sftp oswald Note The configuration file is per-user on your machine - other users will not see your Oswald username unless you share the file. Logging out \u00b6 During an SSH session, you can log out using the exit command, or by closing the connection in any other way.","title":"Accessing Oswald"},{"location":"oswald/quickstart/accessing-oswald/#accessing-oswald","text":"","title":"Accessing Oswald"},{"location":"oswald/quickstart/accessing-oswald/#getting-an-account","text":"To access the Oswald cluster you need to have an account set up by the cluster system administrators. Log a ticket with the IT Helpline to request this and you will be provided with a username and password by email (your 'registration email'). Note Throughout the following documentation, replace <username> with the username you were given in your registration email.","title":"Getting an Account"},{"location":"oswald/quickstart/accessing-oswald/#logging-in","text":"","title":"Logging In"},{"location":"oswald/quickstart/accessing-oswald/#ssh-clients","text":"Like most supercomputers, Oswald allows you to log in using Secure Shell (SSH). SSH is available for various operating systems: Unix/Linux : The OpenSSH client is pre-installed on many Linux distributions. If the OpenSSH client is not installed on your system: If using your own machine, you can install it with your distribution's package manager (eg. sudo apt install openssh-client on Ubuntu-based systems). If using a university machine, log a ticket with the IT Helpline to have it set up on your machine. Once installed, log in using: ssh <username>@oswald Mac : The OpenSSH client is pre-installed on most Macs. Log in using: ssh <username>@oswald Windows : Multiple applications for Windows allow using SSH, including: Putty MobaXterm If none of this software is installed on your machine: If using your own machine, you can download and install the application of your choice from the pages linked above. If using a university machine, log a ticket with the IT Helpline to have the required software set up on your machine. Refer to the documentation for these applications for how to connect using them. The 'server address' you need to connect to is oswald . Todo How can users connect to Oswald from outside the university, where the oswald hostname is not already set up? Note : Remember to also update Configuring SSH below. Todo Add detail about setting up graphical ssh clients for Oswald.","title":"SSH Clients"},{"location":"oswald/quickstart/accessing-oswald/#security-and-authentication","text":"After running the above command (or before or while attempting to log in through your SSH client application), you will be prompted for a password. You should use the password you received in your registration email. The first time you log in, you may also be asked to accept the \"host key\" of Oswald's head node. You can accept this.","title":"Security and Authentication"},{"location":"oswald/quickstart/accessing-oswald/#configuring-ssh","text":"When using the above Unix/Linux/Mac commands, having to repeat them every time you wish to log in or transfer files can become tedious. OpenSSH allows using a configuration file (usually located at ~/.ssh/config ) to save some of the information for future logins. In that file, you can add the following: Host oswald HostName oswald User <username> After that, you can provide the server address (which usually looks like <username>@oswald ) as simply oswald , such as in: ssh oswald scp <file-name> oswald:<destination-directory> sftp oswald Note The configuration file is per-user on your machine - other users will not see your Oswald username unless you share the file.","title":"Configuring SSH"},{"location":"oswald/quickstart/accessing-oswald/#logging-out","text":"During an SSH session, you can log out using the exit command, or by closing the connection in any other way.","title":"Logging out"},{"location":"oswald/quickstart/application-development-tools/","text":"Application Development Tools \u00b6 Oswald includes the GNU and Intel compilers for software development. The GNU compilers are general compilers that produce executable files that can run on any computer architecture. The Intel compilers are designed and optimised for Intel architectures - executable files created by them can only run on Intel CPUs (which Oswald uses), but may be smaller and run faster than programs created from the same source code using the GNU compilers. See their individual pages for full details of the names of their modules and the commands they provide. Warning While building an application, you must use the same compiler and libraries (whether pre-built/pre-installed, or written yourself) throughout the build. Mixing library versions or compilers (vendors or versions), including using libraries compiled with a different compiler/compiler version, may result in problems compiling your application, or runtime problems even if it compiles.","title":"Application Development Tools"},{"location":"oswald/quickstart/application-development-tools/#application-development-tools","text":"Oswald includes the GNU and Intel compilers for software development. The GNU compilers are general compilers that produce executable files that can run on any computer architecture. The Intel compilers are designed and optimised for Intel architectures - executable files created by them can only run on Intel CPUs (which Oswald uses), but may be smaller and run faster than programs created from the same source code using the GNU compilers. See their individual pages for full details of the names of their modules and the commands they provide. Warning While building an application, you must use the same compiler and libraries (whether pre-built/pre-installed, or written yourself) throughout the build. Mixing library versions or compilers (vendors or versions), including using libraries compiled with a different compiler/compiler version, may result in problems compiling your application, or runtime problems even if it compiles.","title":"Application Development Tools"},{"location":"oswald/quickstart/running-jobs/","text":"Jobs \u00b6 Work on the cluster is submitted as a job to a workload manager which allocates the job to one or more compute nodes, depending on the requested and available resources. Jobs are held in a job queue until sufficient resources are available on the cluster to run the job. Several workload managers exist for different supercomputers, such as PBSPro, Torque, SGE, and SLURM. Oswald uses the SLURM ('Simple Linux Utility for Resource Management') workload manager. Warning Working on the head node is only permitted for setting up job scripts, submitting jobs, and editing and compiling source code. Though technically possible, HPC applications (whether pre-installed on Oswald, aquired from an external source, or written/compiled yourself) should never be run on the head node, even for testing. If you need to test your application or job, you should run it on the cluster's debug queue. See Job Queues/Partitions and Limits for details. GUI applications also cannot be run on the head node, even for the valid purposes listed above. Using graphical applications for writing code or jobscripts, or creating small amounts of data, must either be done on your own machine (and the files copied onto oswald), or done on the visualisation node. For visualising your data/results, or other uses of GUI applications that involve manipulating large data sets, you should use the visualisation node. This avoids slow data transfers to and from Oswald and allows you to analyse very large data sets that you could not store on your own machine. For how to use the visualisation node, see Visualisation Node . To create a job, a job script must be created and submitted to SLURM. SLURM job scripts may include a range of options to customise their behaviour. Once the job is submitted, it can be monitored and cancelled. The rest of this page describes each of these stages in more detail. Job Scripts \u00b6 A job script is a shell script in which you specify the command(s) to run when the job is run, as well as setting various options for the job and other application-specific environment variables. A job script has the following basic structure: Shebang Line : A single line that specifies the shell to use (often #!/bin/sh or #!/bin/bash ). SLURM Directives : Optional lines that SLURM uses to set properties about the job, such as the number of nodes to use, the number of processor cores to use, memory requirements, etc. These can either have a 'long' format: #SBATCH --<option>=<parameter> , or a short format: #SBATCH -<o> <parameter> (where <o> is the option's single-character name). In either case, leave no space between the # and SBATCH , otherwise the directive will be treated as a comment and ignored. Many common options and what they do are listed below . Setup : Optional shell commands to load required modules and define application-specific environment variables. Application Execution : The shell command(s) to execute the application. This could include running software you have downloaded to assist in your research, an application you have written yourself, a script that uses pre-installed packages, or a combination of these. Garbage Collection : The shell commands to clean up any temporary workfiles generated during the job execution to leave the node(s) clean for the next job. Example Assume we have created a simple MPI-based C program: /* Hello World using MPI */ // include the MPI header files #include <stdio.h> #include <stdlib.h> #include <mpi.h> int main(int argc, char **argv) { int rank, size, name_len; char hostname[20]; // Initialise the MPI environment MPI_Init(&argc, &argv); // Get the number of processes MPI_Comm_size(MPI_COMM_WORLD, &size); // Get the rank of the process MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get the name of the node MPI_Get_processor_name(hostname, &name_len); // Print Hello World message from the process printf(\"Hello World from node %s rank %d out of %d\\n\", hostname, rank, size); //Finalize the MPI environment MPI_Finalize(); return 0; } Note If you wish to follow along with this example, the program above is available for you to copy at /home/EXAMPLES/HelloWorld/hello-mpi.c on Oswald, or you can paste the above code into your favourite text editor and save it as a .c file. We will use the GNU MPI compiler with the OpenMPI libraries to compile and build the executable. The GNU MPI compiler is provided by the OpenMPI module. First we need to load the OpenMPI library module into our environment: module load openmpi/gcc/64/1.10.0-hfi Note We need to load the module with the -hfi suffix to use the OmniPath Interconnect. See MPI for details. Next, compile the source code using the GNU MPI C compiler ( mpicc ): mpicc hello-mpi.c -o hello-mpi Finally, below is an example job script, named slurm-hello-mpi.sh , to run the 'Hello World' application on the cluster. #!/bin/sh # Configure the SBATCH directives for this job # Set Job Name to be displayed in the job queue #SBATCH --job-name=Hello-MPI # Specify where the output of this job will be stored # %j = job number #SBATCH --output=hello-mpi-%j.out # Set maximum wallclock time limit for this job # Time format = days-hours:minutes:seconds #SBATCH --time=0-00:30:00 # Set number of nodes to use #SBATCH --nodes=2 # Set number of tasks to run per node (tasks=cores/threads) #SBATCH --ntasks-per-node=10 # Set the partition/queue to submit the job to #SBATCH --partition=24hour # Set email address to send email notifications #SBATCH --mail-user=your.email.address@here # Send an email when job has started, finished or aborted #SBATCH --mail-type=ALL # Setup # Load the modules required by this job module load slurm openmpi/gcc/64/1.10.2-hfi # Run the application using 20 process slots mpirun -n 20 hello-mpi Note The example job script above is available for you to copy at /home/EXAMPLES/HelloWorld/slurm-hello-mpi.sh on Oswald, or you can paste the above code into your favourite text editor and save it as a .sh file. The job script above is configured so that when it is submitted, it is submitted to the 24-hour queue ( --partition=24hour ). Then, once suffient resources are available on the cluster, SLURM will run the job over 2 nodes ( --nodes=2 ) with 10 tasks per node ( --ntasks=10 ), for a total of 20 process slots. The mpirun command will run the given application ( hello-mpi ) over the specified number of process slots ( -n 20 ). Here, 20 makes it fill all 20 slots the job script requests. The job script also specifies that it will run for no more than 30 minutes ( --time=0-00:30:00 ), and to notify you ( --mail-user=your.email.address@here ) when it has started and completed, or failed ( --mail-type=ALL ). Once it has completed, you will be able to access the output of the job in a file whose name is in the format hello-mpi-<jobID>.out , eg. hello-mpi-20220.out . Note You should try to use as many resources as you request and only request as many resources as you need to avoid inefficiency in the job allocation system. Reserving resources you don't need only adds to the time other users have to wait for their jobs to start, as resources (CPU cores, RAM) that are reserved by your job but are not being used cannot be used by other jobs for as long as your job is running. It is rare that jobs will need to specify a RAM usage limit, but it is possible. Nodes are shared by default ('oversubscribed' in SLURM terminology), meaning different jobs can reserve their own reservable resources (CPU cores, RAM) on the same node at the same time . This can be overridden by specifying exclusive mode for a job using the --exclusive option to SBATCH in a job script, or in the sbatch command. Exclusive mode will make all nodes that job runs on reserved by that job, even if those nodes have spare resources (CPU cores, RAM) that are not being used by that job. As such, exclusive mode should only be used if the application the job is running requires it. SLURM Directives \u00b6 The following is a list of common directives. Note Lists of possible values for directives given here may not be comprehensive. A full list of all directives and possible parameter values can be seen by running sbatch -help , or running sbatch -usage for a more concise list. Option (long and short format) Description and Default Value #SBATCH --jobname=<jobname> #SBATCH -J <jobname> Set the name of the job. Default : The name of the job script. #SBATCH --partition=<partition> #SBATCH -p <partition> Run the job in the specified partition/queue. Default : debug (for Oswald). #SBATCH --nodes=<num-nodes> #SBATCH -N <num-nodes> Request the specified number of nodes. Default : As many nodes as needed to provide for the number of tasks. #SBATCH --ntasks=<num-tasks> #SBATCH -n <num-tasks> Request the specified number of tasks (usually equivalent to CPU cores) per node. Default : 1 task per node (so 1 task in total if --nodes is also not provided in the job script). #SBATCH --time=<time> * #SBATCH -t <time> Request the maximum amount of time the task will need to run. This can be no more than the maximum time limit of the partition used. Default : The maximum time limit of the partition used. #SBATCH --begin=<time> * Request the job to start at a specific time. Default : As soon as the necessary resources become available. #SBATCH --workdir=<directory-path> #SBATCH -D <directory-path> Set the working directory to run the script in. Default : The working directory of the shell at the time it ran sbatch to submit the job. #SBATCH --error=<filename> #SBATCH -e <filename> Set the error log file name. This can be a relative path from your current directory. Default : slurm-<jobID>.out #SBATCH --output=<filename> #SBATCH -o <filename> Set the output log file name. This can be a relative path from your current directory. Default : slurm-<jobID>.out #SBATCH --mail-user=<user@address> Set the email address for job notifications - use your email address. Default : Do not notify. #SBATCH --mail-type=<BEGIN, END, FAIL, ALL> Set the type(s) of job notification to send - usually one of BEGIN , END , FAIL , or ALL . Default : Do not notify. * Durations are given in the format hours:minutes:seconds or days-hours:minutes:seconds . It is recommended that in every job script you at least specify the following: Number of tasks : This will depend on your job. Maximum runtime : This may require an educated guess or experience from previous job submissions. If in doubt, where you want to run several similar jobs, use the maximum for the relevant job queue for the first job submission, then use a reasonable value for the remaining job submissions based on how long the first one took to run. You can find out how long a job took to run by turning on email notifications for when the job starts and completes. Partition : Select a suitable partition based on the number of nodes, tasks, and time needed. Job Queues/Partitions and Limits \u00b6 Oswald has several job queues ('partitions' in SLURM terminology), with different maximum 'wallclock' (actual/real) run times and maximum number of nodes per job, as in the table below: Queue/Partition Name Max Number of Nodes Max Wallclock Runtime 120 hour 4 120 hours / 5 days 72 hour 4 72 hours / 3 days 48 hour 8 48 hours / 2 days 24 hour 24 24 hours / 1 day debug 1 1 hour When writing job scripts, you should specify to use the job queue with the shortest maximum wallclock time that is more than your job's maximum time. Similarly to setting appropriate resource limits (as mentioned above), this helps to avoid inefficiency in the job allocation system. When specifying the job queue to use (using #SBATCH --partition ), do not put spaces in the queue name (eg. use 24hour , not 24 hour ). Example Minimal job script : If you had a job that you know won't run for more than 60 hours (eg. it previously ran for 52:35:23) and only requires 80 tasks/cores (ie. a minimum of 3 nodes), then in your job script you could specify (as a minimum) something like: #!/bin/bash #SBATCH --ntasks 80 #SBATCH --time 60:00:00 #SBATCH --partition 72hour srun -n 80 your_desired_program_or_script Todo Check that this minimal example isn't against Oswald's policies on SLURM usage. Note The debug queue is meant to be used as a quick check to see if your code and/or script are configured correctly. It is the default queue if you do not specify another queue using the -p or --partition #SBATCH directive in your job script, or on the command-line using the --partition option, like so: sbatch --partition=48hour slurm-hello-mpi.sh Normal jobs should specify one of the other queues , as appropriate for the job's requirements. Warning If the requested number of nodes ( --nodes ) exceeds the partition's maximum number of nodes, or the requested time limit of a job script ( --time ) exceeds the partition's maximum time limit, then the job will be left in a PENDING state (possibly indefinitely), meaning it will not run. Submitting a Job \u00b6 Once a job script is made it can be submitted to SLURM to be run on the cluster using the sbatch command: $ sbatch <script-name> It will then output a line that looks like: Submitted batch job XXXXX Where XXXXX is the job number assigned to your submitted job - you can use this to manage the job after it has been submitted. Example Continuing on from the previous example: $ sbatch slurm-hello-mpi.sh Submitted batch job 20220 Monitoring a Job \u00b6 You can monitor the progress of your job using the squeue command. This will state the following information about every job currently running on the cluster: Job ID Partition Job name Initiating user Current status - usually either pending ( PD ) or running ( R ) Wallclock running time - given in the format days-hours:minutes:seconds Number of nodes being used Exact list of nodes reserved - ranges are given in square brackets Example Assuming our job using the hello-world-mpi.sh job script was submitted with the job ID 20220, squeue might output the following (our job is the last job listed): $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 20211 24hour slurm-he jimbob PD 0:00 2 (Resources) 20165 120hour MOS_Soil leanneW R 1-17:54:19 3 compute[022-024] 20164 120hour SNI_Soil leanneW R 1-17:55:26 3 compute[019-021] 20186 120hour Dots3D58 elfegoG R 3:08:27 1 compute010 20070 120hour MATLAB-p jimbob R 4-06:08:07 1 compute001 20220 24hour Hello-MP jimbob R 0:01 2 compute[016-017] This shows our job has been running on 2 nodes ( compute016 and compute017 ) for 1 second. Note that job 20211 is being held in a pending state ( PD ) as the requested resources are not available - the job will start running once the requested resources are available. You will be sent an email when your job starts/ends/fails according to the setting of the --mail-user and --mail-type #SBATCH directives in your job script. Tip This feature is very useful when you submit a large job which may take days to complete - you can logout, leaving your job running, and return when you receive the email that your job has finished, hopefully successfully. Example In the 'Hello World' example, the emails will be sent almost simultaneously, as the job completes almost instantly (it is only making a file containing \"Hello World from node X rank Y out of Z\" 20 times, after all!). Job Output \u00b6 When the job has finished, the job details will disappear from the queue (and output of squeue ) and you can then examine the output file produced (if one is produced by your job). The file will be named whatever you specified in the #SBATCH --output <filename.log> directive, and will be located in your current directory. If you have logged out and logged back in since submitting the job, the file will be located in whichever directory was your current directory at the time you submitted the job with sbatch . Example The output file for the 'Hello World' job is hello-mpi-20220.out . As we expect it to be short, we can inspect it with the cat command: $ cat hello-mpi-20220.out Hello World from node compute016 process 4 out of 20 Hello World from node compute016 process 5 out of 20 Hello World from node compute016 process 6 out of 20 Hello World from node compute016 process 8 out of 20 Hello World from node compute016 process 9 out of 20 Hello World from node compute016 process 2 out of 20 Hello World from node compute016 process 3 out of 20 Hello World from node compute017 process 14 out of 20 Hello World from node compute016 process 0 out of 20 Hello World from node compute016 process 1 out of 20 Hello World from node compute016 process 7 out of 20 Hello World from node compute017 process 15 out of 20 Hello World from node compute017 process 16 out of 20 Hello World from node compute017 process 18 out of 20 Hello World from node compute017 process 19 out of 20 Hello World from node compute017 process 10 out of 20 Hello World from node compute017 process 12 out of 20 Hello World from node compute017 process 11 out of 20 Hello World from node compute017 process 13 out of 20 Hello World from node compute017 process 17 out of 20 Note As this is an MPI application, the output file is added to by each of the processes/tasks running on the requested nodes independently of one another (i.e. no process/task depends waits for any other process/task to output its line). As such, the output of this job probably won't be in sequential order - the messages appear in the order in which the processes/tasks run and print the text (which will appear to be almost arbitrary after the job has started). Cancelling a Job \u00b6 You may realise after submitting a job that something is wrong with it, eg. you put it on the wrong queue, set the wrong number of cores, or otherwise misconfigured parameters, so you need to cancel the job. You can cancel a job using the scancel command, giving the job ID of the job you wish to cancel: scancel <jobID> Example If we wanted to cancel the 'Hello World' job with ID 20220, we would run: scancel 20220 Note You can only cancel your own jobs. System administrators can cancel any job. More Information \u00b6 More information about the SLURM workload manager and SLURM commands can be found on the SLURM docs , or in the relevant man pages (eg. man sbatch ).","title":"Running Jobs"},{"location":"oswald/quickstart/running-jobs/#jobs","text":"Work on the cluster is submitted as a job to a workload manager which allocates the job to one or more compute nodes, depending on the requested and available resources. Jobs are held in a job queue until sufficient resources are available on the cluster to run the job. Several workload managers exist for different supercomputers, such as PBSPro, Torque, SGE, and SLURM. Oswald uses the SLURM ('Simple Linux Utility for Resource Management') workload manager. Warning Working on the head node is only permitted for setting up job scripts, submitting jobs, and editing and compiling source code. Though technically possible, HPC applications (whether pre-installed on Oswald, aquired from an external source, or written/compiled yourself) should never be run on the head node, even for testing. If you need to test your application or job, you should run it on the cluster's debug queue. See Job Queues/Partitions and Limits for details. GUI applications also cannot be run on the head node, even for the valid purposes listed above. Using graphical applications for writing code or jobscripts, or creating small amounts of data, must either be done on your own machine (and the files copied onto oswald), or done on the visualisation node. For visualising your data/results, or other uses of GUI applications that involve manipulating large data sets, you should use the visualisation node. This avoids slow data transfers to and from Oswald and allows you to analyse very large data sets that you could not store on your own machine. For how to use the visualisation node, see Visualisation Node . To create a job, a job script must be created and submitted to SLURM. SLURM job scripts may include a range of options to customise their behaviour. Once the job is submitted, it can be monitored and cancelled. The rest of this page describes each of these stages in more detail.","title":"Jobs"},{"location":"oswald/quickstart/running-jobs/#job-scripts","text":"A job script is a shell script in which you specify the command(s) to run when the job is run, as well as setting various options for the job and other application-specific environment variables. A job script has the following basic structure: Shebang Line : A single line that specifies the shell to use (often #!/bin/sh or #!/bin/bash ). SLURM Directives : Optional lines that SLURM uses to set properties about the job, such as the number of nodes to use, the number of processor cores to use, memory requirements, etc. These can either have a 'long' format: #SBATCH --<option>=<parameter> , or a short format: #SBATCH -<o> <parameter> (where <o> is the option's single-character name). In either case, leave no space between the # and SBATCH , otherwise the directive will be treated as a comment and ignored. Many common options and what they do are listed below . Setup : Optional shell commands to load required modules and define application-specific environment variables. Application Execution : The shell command(s) to execute the application. This could include running software you have downloaded to assist in your research, an application you have written yourself, a script that uses pre-installed packages, or a combination of these. Garbage Collection : The shell commands to clean up any temporary workfiles generated during the job execution to leave the node(s) clean for the next job. Example Assume we have created a simple MPI-based C program: /* Hello World using MPI */ // include the MPI header files #include <stdio.h> #include <stdlib.h> #include <mpi.h> int main(int argc, char **argv) { int rank, size, name_len; char hostname[20]; // Initialise the MPI environment MPI_Init(&argc, &argv); // Get the number of processes MPI_Comm_size(MPI_COMM_WORLD, &size); // Get the rank of the process MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get the name of the node MPI_Get_processor_name(hostname, &name_len); // Print Hello World message from the process printf(\"Hello World from node %s rank %d out of %d\\n\", hostname, rank, size); //Finalize the MPI environment MPI_Finalize(); return 0; } Note If you wish to follow along with this example, the program above is available for you to copy at /home/EXAMPLES/HelloWorld/hello-mpi.c on Oswald, or you can paste the above code into your favourite text editor and save it as a .c file. We will use the GNU MPI compiler with the OpenMPI libraries to compile and build the executable. The GNU MPI compiler is provided by the OpenMPI module. First we need to load the OpenMPI library module into our environment: module load openmpi/gcc/64/1.10.0-hfi Note We need to load the module with the -hfi suffix to use the OmniPath Interconnect. See MPI for details. Next, compile the source code using the GNU MPI C compiler ( mpicc ): mpicc hello-mpi.c -o hello-mpi Finally, below is an example job script, named slurm-hello-mpi.sh , to run the 'Hello World' application on the cluster. #!/bin/sh # Configure the SBATCH directives for this job # Set Job Name to be displayed in the job queue #SBATCH --job-name=Hello-MPI # Specify where the output of this job will be stored # %j = job number #SBATCH --output=hello-mpi-%j.out # Set maximum wallclock time limit for this job # Time format = days-hours:minutes:seconds #SBATCH --time=0-00:30:00 # Set number of nodes to use #SBATCH --nodes=2 # Set number of tasks to run per node (tasks=cores/threads) #SBATCH --ntasks-per-node=10 # Set the partition/queue to submit the job to #SBATCH --partition=24hour # Set email address to send email notifications #SBATCH --mail-user=your.email.address@here # Send an email when job has started, finished or aborted #SBATCH --mail-type=ALL # Setup # Load the modules required by this job module load slurm openmpi/gcc/64/1.10.2-hfi # Run the application using 20 process slots mpirun -n 20 hello-mpi Note The example job script above is available for you to copy at /home/EXAMPLES/HelloWorld/slurm-hello-mpi.sh on Oswald, or you can paste the above code into your favourite text editor and save it as a .sh file. The job script above is configured so that when it is submitted, it is submitted to the 24-hour queue ( --partition=24hour ). Then, once suffient resources are available on the cluster, SLURM will run the job over 2 nodes ( --nodes=2 ) with 10 tasks per node ( --ntasks=10 ), for a total of 20 process slots. The mpirun command will run the given application ( hello-mpi ) over the specified number of process slots ( -n 20 ). Here, 20 makes it fill all 20 slots the job script requests. The job script also specifies that it will run for no more than 30 minutes ( --time=0-00:30:00 ), and to notify you ( --mail-user=your.email.address@here ) when it has started and completed, or failed ( --mail-type=ALL ). Once it has completed, you will be able to access the output of the job in a file whose name is in the format hello-mpi-<jobID>.out , eg. hello-mpi-20220.out . Note You should try to use as many resources as you request and only request as many resources as you need to avoid inefficiency in the job allocation system. Reserving resources you don't need only adds to the time other users have to wait for their jobs to start, as resources (CPU cores, RAM) that are reserved by your job but are not being used cannot be used by other jobs for as long as your job is running. It is rare that jobs will need to specify a RAM usage limit, but it is possible. Nodes are shared by default ('oversubscribed' in SLURM terminology), meaning different jobs can reserve their own reservable resources (CPU cores, RAM) on the same node at the same time . This can be overridden by specifying exclusive mode for a job using the --exclusive option to SBATCH in a job script, or in the sbatch command. Exclusive mode will make all nodes that job runs on reserved by that job, even if those nodes have spare resources (CPU cores, RAM) that are not being used by that job. As such, exclusive mode should only be used if the application the job is running requires it.","title":"Job Scripts"},{"location":"oswald/quickstart/running-jobs/#slurm-directives","text":"The following is a list of common directives. Note Lists of possible values for directives given here may not be comprehensive. A full list of all directives and possible parameter values can be seen by running sbatch -help , or running sbatch -usage for a more concise list. Option (long and short format) Description and Default Value #SBATCH --jobname=<jobname> #SBATCH -J <jobname> Set the name of the job. Default : The name of the job script. #SBATCH --partition=<partition> #SBATCH -p <partition> Run the job in the specified partition/queue. Default : debug (for Oswald). #SBATCH --nodes=<num-nodes> #SBATCH -N <num-nodes> Request the specified number of nodes. Default : As many nodes as needed to provide for the number of tasks. #SBATCH --ntasks=<num-tasks> #SBATCH -n <num-tasks> Request the specified number of tasks (usually equivalent to CPU cores) per node. Default : 1 task per node (so 1 task in total if --nodes is also not provided in the job script). #SBATCH --time=<time> * #SBATCH -t <time> Request the maximum amount of time the task will need to run. This can be no more than the maximum time limit of the partition used. Default : The maximum time limit of the partition used. #SBATCH --begin=<time> * Request the job to start at a specific time. Default : As soon as the necessary resources become available. #SBATCH --workdir=<directory-path> #SBATCH -D <directory-path> Set the working directory to run the script in. Default : The working directory of the shell at the time it ran sbatch to submit the job. #SBATCH --error=<filename> #SBATCH -e <filename> Set the error log file name. This can be a relative path from your current directory. Default : slurm-<jobID>.out #SBATCH --output=<filename> #SBATCH -o <filename> Set the output log file name. This can be a relative path from your current directory. Default : slurm-<jobID>.out #SBATCH --mail-user=<user@address> Set the email address for job notifications - use your email address. Default : Do not notify. #SBATCH --mail-type=<BEGIN, END, FAIL, ALL> Set the type(s) of job notification to send - usually one of BEGIN , END , FAIL , or ALL . Default : Do not notify. * Durations are given in the format hours:minutes:seconds or days-hours:minutes:seconds . It is recommended that in every job script you at least specify the following: Number of tasks : This will depend on your job. Maximum runtime : This may require an educated guess or experience from previous job submissions. If in doubt, where you want to run several similar jobs, use the maximum for the relevant job queue for the first job submission, then use a reasonable value for the remaining job submissions based on how long the first one took to run. You can find out how long a job took to run by turning on email notifications for when the job starts and completes. Partition : Select a suitable partition based on the number of nodes, tasks, and time needed.","title":"SLURM Directives"},{"location":"oswald/quickstart/running-jobs/#job-queuespartitions-and-limits","text":"Oswald has several job queues ('partitions' in SLURM terminology), with different maximum 'wallclock' (actual/real) run times and maximum number of nodes per job, as in the table below: Queue/Partition Name Max Number of Nodes Max Wallclock Runtime 120 hour 4 120 hours / 5 days 72 hour 4 72 hours / 3 days 48 hour 8 48 hours / 2 days 24 hour 24 24 hours / 1 day debug 1 1 hour When writing job scripts, you should specify to use the job queue with the shortest maximum wallclock time that is more than your job's maximum time. Similarly to setting appropriate resource limits (as mentioned above), this helps to avoid inefficiency in the job allocation system. When specifying the job queue to use (using #SBATCH --partition ), do not put spaces in the queue name (eg. use 24hour , not 24 hour ). Example Minimal job script : If you had a job that you know won't run for more than 60 hours (eg. it previously ran for 52:35:23) and only requires 80 tasks/cores (ie. a minimum of 3 nodes), then in your job script you could specify (as a minimum) something like: #!/bin/bash #SBATCH --ntasks 80 #SBATCH --time 60:00:00 #SBATCH --partition 72hour srun -n 80 your_desired_program_or_script Todo Check that this minimal example isn't against Oswald's policies on SLURM usage. Note The debug queue is meant to be used as a quick check to see if your code and/or script are configured correctly. It is the default queue if you do not specify another queue using the -p or --partition #SBATCH directive in your job script, or on the command-line using the --partition option, like so: sbatch --partition=48hour slurm-hello-mpi.sh Normal jobs should specify one of the other queues , as appropriate for the job's requirements. Warning If the requested number of nodes ( --nodes ) exceeds the partition's maximum number of nodes, or the requested time limit of a job script ( --time ) exceeds the partition's maximum time limit, then the job will be left in a PENDING state (possibly indefinitely), meaning it will not run.","title":"Job Queues/Partitions and Limits"},{"location":"oswald/quickstart/running-jobs/#submitting-a-job","text":"Once a job script is made it can be submitted to SLURM to be run on the cluster using the sbatch command: $ sbatch <script-name> It will then output a line that looks like: Submitted batch job XXXXX Where XXXXX is the job number assigned to your submitted job - you can use this to manage the job after it has been submitted. Example Continuing on from the previous example: $ sbatch slurm-hello-mpi.sh Submitted batch job 20220","title":"Submitting a Job"},{"location":"oswald/quickstart/running-jobs/#monitoring-a-job","text":"You can monitor the progress of your job using the squeue command. This will state the following information about every job currently running on the cluster: Job ID Partition Job name Initiating user Current status - usually either pending ( PD ) or running ( R ) Wallclock running time - given in the format days-hours:minutes:seconds Number of nodes being used Exact list of nodes reserved - ranges are given in square brackets Example Assuming our job using the hello-world-mpi.sh job script was submitted with the job ID 20220, squeue might output the following (our job is the last job listed): $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 20211 24hour slurm-he jimbob PD 0:00 2 (Resources) 20165 120hour MOS_Soil leanneW R 1-17:54:19 3 compute[022-024] 20164 120hour SNI_Soil leanneW R 1-17:55:26 3 compute[019-021] 20186 120hour Dots3D58 elfegoG R 3:08:27 1 compute010 20070 120hour MATLAB-p jimbob R 4-06:08:07 1 compute001 20220 24hour Hello-MP jimbob R 0:01 2 compute[016-017] This shows our job has been running on 2 nodes ( compute016 and compute017 ) for 1 second. Note that job 20211 is being held in a pending state ( PD ) as the requested resources are not available - the job will start running once the requested resources are available. You will be sent an email when your job starts/ends/fails according to the setting of the --mail-user and --mail-type #SBATCH directives in your job script. Tip This feature is very useful when you submit a large job which may take days to complete - you can logout, leaving your job running, and return when you receive the email that your job has finished, hopefully successfully. Example In the 'Hello World' example, the emails will be sent almost simultaneously, as the job completes almost instantly (it is only making a file containing \"Hello World from node X rank Y out of Z\" 20 times, after all!).","title":"Monitoring a Job"},{"location":"oswald/quickstart/running-jobs/#job-output","text":"When the job has finished, the job details will disappear from the queue (and output of squeue ) and you can then examine the output file produced (if one is produced by your job). The file will be named whatever you specified in the #SBATCH --output <filename.log> directive, and will be located in your current directory. If you have logged out and logged back in since submitting the job, the file will be located in whichever directory was your current directory at the time you submitted the job with sbatch . Example The output file for the 'Hello World' job is hello-mpi-20220.out . As we expect it to be short, we can inspect it with the cat command: $ cat hello-mpi-20220.out Hello World from node compute016 process 4 out of 20 Hello World from node compute016 process 5 out of 20 Hello World from node compute016 process 6 out of 20 Hello World from node compute016 process 8 out of 20 Hello World from node compute016 process 9 out of 20 Hello World from node compute016 process 2 out of 20 Hello World from node compute016 process 3 out of 20 Hello World from node compute017 process 14 out of 20 Hello World from node compute016 process 0 out of 20 Hello World from node compute016 process 1 out of 20 Hello World from node compute016 process 7 out of 20 Hello World from node compute017 process 15 out of 20 Hello World from node compute017 process 16 out of 20 Hello World from node compute017 process 18 out of 20 Hello World from node compute017 process 19 out of 20 Hello World from node compute017 process 10 out of 20 Hello World from node compute017 process 12 out of 20 Hello World from node compute017 process 11 out of 20 Hello World from node compute017 process 13 out of 20 Hello World from node compute017 process 17 out of 20 Note As this is an MPI application, the output file is added to by each of the processes/tasks running on the requested nodes independently of one another (i.e. no process/task depends waits for any other process/task to output its line). As such, the output of this job probably won't be in sequential order - the messages appear in the order in which the processes/tasks run and print the text (which will appear to be almost arbitrary after the job has started).","title":"Job Output"},{"location":"oswald/quickstart/running-jobs/#cancelling-a-job","text":"You may realise after submitting a job that something is wrong with it, eg. you put it on the wrong queue, set the wrong number of cores, or otherwise misconfigured parameters, so you need to cancel the job. You can cancel a job using the scancel command, giving the job ID of the job you wish to cancel: scancel <jobID> Example If we wanted to cancel the 'Hello World' job with ID 20220, we would run: scancel 20220 Note You can only cancel your own jobs. System administrators can cancel any job.","title":"Cancelling a Job"},{"location":"oswald/quickstart/running-jobs/#more-information","text":"More information about the SLURM workload manager and SLURM commands can be found on the SLURM docs , or in the relevant man pages (eg. man sbatch ).","title":"More Information"},{"location":"oswald/quickstart/software-environment/","text":"Software Environment and Modules \u00b6 Most of the software you will use on Oswald is installed in what is called a \u201cModules Environment\u201d. On a traditional Unix/Linux system, software and libraries added by users are installed alongside operating system software and libraries in standard directories (such as /bin , /sbin , /usr/bin , /usr/local/bin , /lib , and /usr/lib ). However, this makes it difficult to separate user and system software and install multiple different versions of the same software or library at the same time. A modules environment provides the means to bundle each software or library package into a self contained module so that: System software/libraries are kept separate from user software/libraries. Different versions of the same software or library can be installed side-by-side. Each user can configure their own environment the way they need it. Using Modules \u00b6 Managing software in a modules environment is done with the various sub-commands of the module command. To know what software and libraries are currently in use in your environment, you can use the module list command. This will show something like: $ module list Currently Loaded Modulefiles: 1) gcc/5.2.0 2) slurm/15.08.6 To use a particular piece of software or library, you must load the module for that software into your environment. This is done with module load <module-name> , replacing <module-name> with the name of the module for the software you want to use. A list of module names and the software they contain can be found on the Software List page. As an example, lets say you want to use version 16.0.4 of the Intel compilers. You can load the Intel compiler module using the load sub-command and then list the loaded modules: $ module load intel/compiler/64/16.0.4/2016.4.258 $ module list Currently Loaded Modulefiles: 1) gcc/5.2.0 2) slurm/15.08.6 3) intel/compiler/64/16.0.4/2016.4.258 Now you can use the Intel Compilers, e.g. ifort , icc , icpc . When you have finished using a module, you can remove it from your environment using module unload <module-name> . This command is mainly useful if you want to use a different version of a piece of software or library that does not allow multiple versions to be loaded at once, in which case you must unload the version you are currently using before loading the other version. All modules will automatically be unloaded when you log out. Continuing on from the previous example, to unload the Intel compiler module: $ module unload intel/compiler/64/16.0.4/2016.4.258 $ module list Currently Loaded Modulefiles: 1) gcc/5.2.0 2) slurm/15.08.6 You can unload all modules from your environment using module purge . For example: $ module purge $ module list No Modulefiles Currently Loaded. Initial Modules \u00b6 The initial set of modules loaded into your environment when you log in can be changed using the module init* sub-commands (see Summary of Module Commands for details of each sub-command). Use these commands if you often load a particular module or set of modules to save you from manually loading them every time you log in. Summary of Module Commands \u00b6 General: Command Description module avail Lists all modules that are available to you to use. module list Lists all modules that are currently loaded. module load <module-name> Loads the module with the given name into your current environment. module unload <module-name> Removes (unloads) the module with the given name from your current environment. module purge Removes (unloads) all modules from your current environment. Initial Modules: Command Description module initadd <module-name> Automatically load the given module on login. module initrm <module-name> Stop automatically loading the given module on login. module initlist List all modules currently being loaded on login. module initclear Stop automatically loading any modules on login. Oswald-Specific Software Information \u00b6 The Software List page contains a list of pages containing general information about the software and libraries that are commonly used on Oswald (eg. MPI). These pages also contain specific information about using the software on Oswald . It is recommended to check the pages on the software you want to use before you start using it on Oswald.","title":"Software Environment and Modules"},{"location":"oswald/quickstart/software-environment/#software-environment-and-modules","text":"Most of the software you will use on Oswald is installed in what is called a \u201cModules Environment\u201d. On a traditional Unix/Linux system, software and libraries added by users are installed alongside operating system software and libraries in standard directories (such as /bin , /sbin , /usr/bin , /usr/local/bin , /lib , and /usr/lib ). However, this makes it difficult to separate user and system software and install multiple different versions of the same software or library at the same time. A modules environment provides the means to bundle each software or library package into a self contained module so that: System software/libraries are kept separate from user software/libraries. Different versions of the same software or library can be installed side-by-side. Each user can configure their own environment the way they need it.","title":"Software Environment and Modules"},{"location":"oswald/quickstart/software-environment/#using-modules","text":"Managing software in a modules environment is done with the various sub-commands of the module command. To know what software and libraries are currently in use in your environment, you can use the module list command. This will show something like: $ module list Currently Loaded Modulefiles: 1) gcc/5.2.0 2) slurm/15.08.6 To use a particular piece of software or library, you must load the module for that software into your environment. This is done with module load <module-name> , replacing <module-name> with the name of the module for the software you want to use. A list of module names and the software they contain can be found on the Software List page. As an example, lets say you want to use version 16.0.4 of the Intel compilers. You can load the Intel compiler module using the load sub-command and then list the loaded modules: $ module load intel/compiler/64/16.0.4/2016.4.258 $ module list Currently Loaded Modulefiles: 1) gcc/5.2.0 2) slurm/15.08.6 3) intel/compiler/64/16.0.4/2016.4.258 Now you can use the Intel Compilers, e.g. ifort , icc , icpc . When you have finished using a module, you can remove it from your environment using module unload <module-name> . This command is mainly useful if you want to use a different version of a piece of software or library that does not allow multiple versions to be loaded at once, in which case you must unload the version you are currently using before loading the other version. All modules will automatically be unloaded when you log out. Continuing on from the previous example, to unload the Intel compiler module: $ module unload intel/compiler/64/16.0.4/2016.4.258 $ module list Currently Loaded Modulefiles: 1) gcc/5.2.0 2) slurm/15.08.6 You can unload all modules from your environment using module purge . For example: $ module purge $ module list No Modulefiles Currently Loaded.","title":"Using Modules"},{"location":"oswald/quickstart/software-environment/#initial-modules","text":"The initial set of modules loaded into your environment when you log in can be changed using the module init* sub-commands (see Summary of Module Commands for details of each sub-command). Use these commands if you often load a particular module or set of modules to save you from manually loading them every time you log in.","title":"Initial Modules"},{"location":"oswald/quickstart/software-environment/#summary-of-module-commands","text":"General: Command Description module avail Lists all modules that are available to you to use. module list Lists all modules that are currently loaded. module load <module-name> Loads the module with the given name into your current environment. module unload <module-name> Removes (unloads) the module with the given name from your current environment. module purge Removes (unloads) all modules from your current environment. Initial Modules: Command Description module initadd <module-name> Automatically load the given module on login. module initrm <module-name> Stop automatically loading the given module on login. module initlist List all modules currently being loaded on login. module initclear Stop automatically loading any modules on login.","title":"Summary of Module Commands"},{"location":"oswald/quickstart/software-environment/#oswald-specific-software-information","text":"The Software List page contains a list of pages containing general information about the software and libraries that are commonly used on Oswald (eg. MPI). These pages also contain specific information about using the software on Oswald . It is recommended to check the pages on the software you want to use before you start using it on Oswald.","title":"Oswald-Specific Software Information"},{"location":"oswald/quickstart/storage-and-filesystems/","text":"Storage and Filesystems \u00b6 Available Filesystems \u00b6 Like all HPC clusters, Oswald provides primary, scratch, and high-performance storage areas for you and applications to use: Storage Type Location Node(s) Available On Size Storage Type (speed) Quotas Main storage /home Head, Compute, GPU, Visualisation 185TB HDD (slow) None Scratch storage /local Compute 72GB on compute nodes 1-25; 184GB on compute nodes 26-32 SSD (fast) None High-performance storage /lustre/lzfs Head, Compute, GPU, Visualisation 88TB Lustre (very fast for parallel access) None Important Notes: Each user has their own subdirectory on the main and Lustre filesystems, eg. /home/bob (called the user's \"home directory\") and /lustre/lzfs/bob , to ensure different users do not accidentally interfere with each other's files. Users should not, and in some cases cannot, make files or directories directly in /home or /lustre/lzfs . All users can access the home directories of all other users, though files within each user's home directory are private when created. To share a file with other Oswald users, the owner of the file must set read and navigate permissions for 'other users' on their home directory and any parent directories of the file (by using, eg. chmod o+rx dir_name ), then set the relevant permissions on the file itself (by using, eg. chmod o+rwx some_script.py ). There are currently no per-user quotas for user storage, though some restrictions are expected to be introduced in the future. No backups are kept of any data on any filesystem - users are expected to keep their own backups of any software and important input or generated data that they store on the cluster. When using /local scratch storage on compute nodes, users are responsible for cleaning up after every job to avoid it becoming full. Todo Is /tmp accessible by all, some, or no applications? On which nodes can it be accessed (head, compute, GPU, visualisation)? If it's not available for some apps and/or on some nodes, would some apps that need to use it have problems running? Todo Should each user/job use a subdirectory of /local to avoid usage conflicts? (eg. two jobs using a file with the same name, one job deleting a file being used by another job, etc.) Transferring Files \u00b6 Oswald supports both scp and sftp for transferring files between your machine and Oswald. Clients for these are available for various operating systems: Unix/Linux/Mac : The scp and sftp programs are available directly. Use whichever you find most convenient with: scp <file-name> <username>@oswald:<destination-directory> or sftp <username>@oswald See man scp and man sftp for more information on how to use these programs. Windows : Various applications exist that support either scp , sftp , or both. Common applications include: WinSCP Filezilla MobaXterm If none of this software is installed on your machine: If using your own machine, you can download and install the application of your choice from the pages linked above. If using a university machine, log a ticket with the IT Helpline to have the needed software set up on your machine. Refer to the documentation for these applications for how to use them to transfer files. The 'server address' you need to connect to is oswald .","title":"Storage and Filesystems"},{"location":"oswald/quickstart/storage-and-filesystems/#storage-and-filesystems","text":"","title":"Storage and Filesystems"},{"location":"oswald/quickstart/storage-and-filesystems/#available-filesystems","text":"Like all HPC clusters, Oswald provides primary, scratch, and high-performance storage areas for you and applications to use: Storage Type Location Node(s) Available On Size Storage Type (speed) Quotas Main storage /home Head, Compute, GPU, Visualisation 185TB HDD (slow) None Scratch storage /local Compute 72GB on compute nodes 1-25; 184GB on compute nodes 26-32 SSD (fast) None High-performance storage /lustre/lzfs Head, Compute, GPU, Visualisation 88TB Lustre (very fast for parallel access) None Important Notes: Each user has their own subdirectory on the main and Lustre filesystems, eg. /home/bob (called the user's \"home directory\") and /lustre/lzfs/bob , to ensure different users do not accidentally interfere with each other's files. Users should not, and in some cases cannot, make files or directories directly in /home or /lustre/lzfs . All users can access the home directories of all other users, though files within each user's home directory are private when created. To share a file with other Oswald users, the owner of the file must set read and navigate permissions for 'other users' on their home directory and any parent directories of the file (by using, eg. chmod o+rx dir_name ), then set the relevant permissions on the file itself (by using, eg. chmod o+rwx some_script.py ). There are currently no per-user quotas for user storage, though some restrictions are expected to be introduced in the future. No backups are kept of any data on any filesystem - users are expected to keep their own backups of any software and important input or generated data that they store on the cluster. When using /local scratch storage on compute nodes, users are responsible for cleaning up after every job to avoid it becoming full. Todo Is /tmp accessible by all, some, or no applications? On which nodes can it be accessed (head, compute, GPU, visualisation)? If it's not available for some apps and/or on some nodes, would some apps that need to use it have problems running? Todo Should each user/job use a subdirectory of /local to avoid usage conflicts? (eg. two jobs using a file with the same name, one job deleting a file being used by another job, etc.)","title":"Available Filesystems"},{"location":"oswald/quickstart/storage-and-filesystems/#transferring-files","text":"Oswald supports both scp and sftp for transferring files between your machine and Oswald. Clients for these are available for various operating systems: Unix/Linux/Mac : The scp and sftp programs are available directly. Use whichever you find most convenient with: scp <file-name> <username>@oswald:<destination-directory> or sftp <username>@oswald See man scp and man sftp for more information on how to use these programs. Windows : Various applications exist that support either scp , sftp , or both. Common applications include: WinSCP Filezilla MobaXterm If none of this software is installed on your machine: If using your own machine, you can download and install the application of your choice from the pages linked above. If using a university machine, log a ticket with the IT Helpline to have the needed software set up on your machine. Refer to the documentation for these applications for how to use them to transfer files. The 'server address' you need to connect to is oswald .","title":"Transferring Files"},{"location":"oswald/quickstart/using-specialised-nodes/","text":"Using Specialised Nodes \u00b6 Oswald has some nodes for specialised purposes, including a GPU compute node and a visualisation node. These are explained below. GPU Node \u00b6 Todo How do users use the GPU node? By using the relevant --gpu* options to SLURM? Visualisation Node \u00b6 Todo What is the visualisation node for? How do users use the Visualisation node? By using the relevant --gpu* options to SLURM? Can users run interactive GUI apps on it? How? By using srun directly?","title":"Using Specialised Nodes"},{"location":"oswald/quickstart/using-specialised-nodes/#using-specialised-nodes","text":"Oswald has some nodes for specialised purposes, including a GPU compute node and a visualisation node. These are explained below.","title":"Using Specialised Nodes"},{"location":"oswald/quickstart/using-specialised-nodes/#gpu-node","text":"Todo How do users use the GPU node? By using the relevant --gpu* options to SLURM?","title":"GPU Node"},{"location":"oswald/quickstart/using-specialised-nodes/#visualisation-node","text":"Todo What is the visualisation node for? How do users use the Visualisation node? By using the relevant --gpu* options to SLURM? Can users run interactive GUI apps on it? How? By using srun directly?","title":"Visualisation Node"},{"location":"oswald/software/gnu-compilers/","text":"GNU Compilers \u00b6 The GNU Compiler Collection (GCC) and its various aliases and extensions are available for use on Oswald. Todo What packages (exact names) provide the GCC compilers and any related tools? The compilers for the languages supported on Oswald (though others may work) are as follows: Description Command C compiler gcc C++ compiler g++ Fortran compiler gfortran, f90 MPI C compiler mpicc MPI C++ compiler mpicxx MPI Fortran compiler mpifort, mpif77, mpif90","title":"GNU Compilers"},{"location":"oswald/software/gnu-compilers/#gnu-compilers","text":"The GNU Compiler Collection (GCC) and its various aliases and extensions are available for use on Oswald. Todo What packages (exact names) provide the GCC compilers and any related tools? The compilers for the languages supported on Oswald (though others may work) are as follows: Description Command C compiler gcc C++ compiler g++ Fortran compiler gfortran, f90 MPI C compiler mpicc MPI C++ compiler mpicxx MPI Fortran compiler mpifort, mpif77, mpif90","title":"GNU Compilers"},{"location":"oswald/software/intel-compilers/","text":"Intel Compilers \u00b6 The Intel compilers are available for use on Oswald. Todo What packages (exact names) provide the Intel compilers and any related tools? Warning Modules relating to intel compilers with mic in their name are for use with CPUs that use the Intel MIC architecture, such as the Xeon Phi processors. At the time of writing, Oswald has no nodes of this type in the cluster, so do not use those modules. Using them will lead to unspecified behaviour, possibly including cluster instability. The compilers for the languages supported on Oswald (though others may work) are as follows: Description Command C compiler icc C++ compiler icpc Fortran compiler ifort MPI C compiler mpiicc MPI C++ compiler mpiicpc MPI Fortran compiler mpiifort","title":"Intel Compilers"},{"location":"oswald/software/intel-compilers/#intel-compilers","text":"The Intel compilers are available for use on Oswald. Todo What packages (exact names) provide the Intel compilers and any related tools? Warning Modules relating to intel compilers with mic in their name are for use with CPUs that use the Intel MIC architecture, such as the Xeon Phi processors. At the time of writing, Oswald has no nodes of this type in the cluster, so do not use those modules. Using them will lead to unspecified behaviour, possibly including cluster instability. The compilers for the languages supported on Oswald (though others may work) are as follows: Description Command C compiler icc C++ compiler icpc Fortran compiler ifort MPI C compiler mpiicc MPI C++ compiler mpiicpc MPI Fortran compiler mpiifort","title":"Intel Compilers"},{"location":"oswald/software/mpi/","text":"MPI \u00b6 MPI is a communication protocol for programming parallel computers. It allows the compilation of code which can be used over many processors over (possibly) many compute nodes at the same time. Most MPI implementations consist of a specific set of routines directly callable from programming languages such as C, C++, and Fortran, as well as any other programming languages able to interface with such libraries, including C#, Java, and Python. The advantages of MPI over older message passing libraries are: Portability : MPI has been implemented for almost every distributed memory architecture. Speed : Each implementation is in principle optimized for the hardware on which it runs. There are four MPI-2 implementations available on Oswald: MPICH2 ( mpich2 ) MVAPICH2 ( mvapich2 ) OpenMPI ( open-mpi ) Intel MPI ( intel-mpi ) These libraries can be compiled with GCC, Open64, and Intel compilers. Important Note As Oswald uses an OmniPath (OPA) interconnect (100Gb/s) you should only use the Host Fabric Interface-compatible (HFI) libraries (those with the -hfi suffix), by loading the appropriate module. These have been compiled and optimised to use OmniPath. The other libraries will default to communicate over the management network which is much slower (1Gb/s) and can cause cluster instability.","title":"MPI"},{"location":"oswald/software/mpi/#mpi","text":"MPI is a communication protocol for programming parallel computers. It allows the compilation of code which can be used over many processors over (possibly) many compute nodes at the same time. Most MPI implementations consist of a specific set of routines directly callable from programming languages such as C, C++, and Fortran, as well as any other programming languages able to interface with such libraries, including C#, Java, and Python. The advantages of MPI over older message passing libraries are: Portability : MPI has been implemented for almost every distributed memory architecture. Speed : Each implementation is in principle optimized for the hardware on which it runs. There are four MPI-2 implementations available on Oswald: MPICH2 ( mpich2 ) MVAPICH2 ( mvapich2 ) OpenMPI ( open-mpi ) Intel MPI ( intel-mpi ) These libraries can be compiled with GCC, Open64, and Intel compilers. Important Note As Oswald uses an OmniPath (OPA) interconnect (100Gb/s) you should only use the Host Fabric Interface-compatible (HFI) libraries (those with the -hfi suffix), by loading the appropriate module. These have been compiled and optimised to use OmniPath. The other libraries will default to communicate over the management network which is much slower (1Gb/s) and can cause cluster instability.","title":"MPI"},{"location":"oswald/software/software-list/","text":"Software List \u00b6 Libraries: MPI Compilers: GNU Compilers Intel Compilers","title":"Software List"},{"location":"oswald/software/software-list/#software-list","text":"Libraries: MPI Compilers: GNU Compilers Intel Compilers","title":"Software List"},{"location":"oswald/support/faqs/","text":"FAQs \u00b6 Currently no FAQs.","title":"FAQs"},{"location":"oswald/support/faqs/#faqs","text":"Currently no FAQs.","title":"FAQs"},{"location":"oswald/support/known-issues/","text":"Known Issues \u00b6 Currently no known issues.","title":"Known Issues"},{"location":"oswald/support/known-issues/#known-issues","text":"Currently no known issues.","title":"Known Issues"},{"location":"oswald/support/university-support/","text":"University Support \u00b6 Problems : If you have any problems using the Oswald HPC Cluster that aren't covered in the FAQs or Know Issues please log a ticket with the IT Helpline. Requesting Software : If you require any software installing on the cluster, please log a ticket with the IT helpline. Include software name, download source, version number, and compiler to use for open source software. Please note that there may be a delay in installing your requested software while the relevant licenses are checked by the University Legal Department.","title":"University Support"},{"location":"oswald/support/university-support/#university-support","text":"Problems : If you have any problems using the Oswald HPC Cluster that aren't covered in the FAQs or Know Issues please log a ticket with the IT Helpline. Requesting Software : If you require any software installing on the cluster, please log a ticket with the IT helpline. Include software name, download source, version number, and compiler to use for open source software. Please note that there may be a delay in installing your requested software while the relevant licenses are checked by the University Legal Department.","title":"University Support"}]}